{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = [[1, 0, 0, 0, 0],  #h\n",
    "           [0, 1, 0, 0, 0],  #i\n",
    "           [0, 0, 1, 0, 0],  #e\n",
    "           [0, 0, 0, 1, 0],  #l\n",
    "           [0, 0, 0, 0, 1]]  #0\n",
    "            \n",
    "x_data = [0, 1, 0, 2, 3, 3]  # hihell\n",
    "y_data = [1, 0, 2, 3, 3, 4]  # ihello\n",
    "x_one_hot = [one_hot[x] for x in x_data]\n",
    "\n",
    "getchar = ['h', 'i', 'e', 'l', 'o']\n",
    "            #0   1    2    3    4\n",
    "inputs = (torch.Tensor(x_one_hot))\n",
    "labels = (torch.LongTensor(y_data))\n",
    "\n",
    "input_dim = 5\n",
    "hidden_dim = 5\n",
    "batch_size = 1\n",
    "seq_length = 1\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        \n",
    "    def forward(self, hidden, x):\n",
    "        x = x.view(batch_size, seq_length, input_dim)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out= out.view(-1, 5)\n",
    "        return hidden, out\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(num_layers, batch_size, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:hihhih , epoch: 1, loss: 9.665 \n",
      "String:hihhll , epoch: 2, loss: 8.809 \n",
      "String:hihhll , epoch: 3, loss: 8.356 \n",
      "String:hlehll , epoch: 4, loss: 7.999 \n",
      "String:hhehlo , epoch: 5, loss: 7.686 \n",
      "String:hhehlo , epoch: 6, loss: 7.405 \n",
      "String:hhehlo , epoch: 7, loss: 7.150 \n",
      "String:hhehlo , epoch: 8, loss: 6.919 \n",
      "String:hhehlo , epoch: 9, loss: 6.708 \n",
      "String:hhehlo , epoch: 10, loss: 6.515 \n",
      "String:ihehlo , epoch: 11, loss: 6.336 \n",
      "String:ihehlo , epoch: 12, loss: 6.170 \n",
      "String:ihehlo , epoch: 13, loss: 6.014 \n",
      "String:ihehlo , epoch: 14, loss: 5.868 \n",
      "String:ihehlo , epoch: 15, loss: 5.731 \n",
      "String:ihehlo , epoch: 16, loss: 5.602 \n",
      "String:ihehlo , epoch: 17, loss: 5.480 \n",
      "String:ihehlo , epoch: 18, loss: 5.366 \n",
      "String:ihehlo , epoch: 19, loss: 5.258 \n",
      "String:ihehlo , epoch: 20, loss: 5.156 \n",
      "String:ihehlo , epoch: 21, loss: 5.059 \n",
      "String:ihehlo , epoch: 22, loss: 4.969 \n",
      "String:ihehlo , epoch: 23, loss: 4.883 \n",
      "String:ihehlo , epoch: 24, loss: 4.802 \n",
      "String:ihehlo , epoch: 25, loss: 4.725 \n",
      "String:ihehlo , epoch: 26, loss: 4.653 \n",
      "String:ihehlo , epoch: 27, loss: 4.585 \n",
      "String:ihehlo , epoch: 28, loss: 4.521 \n",
      "String:ihehlo , epoch: 29, loss: 4.461 \n",
      "String:ihehlo , epoch: 30, loss: 4.404 \n",
      "String:ihehlo , epoch: 31, loss: 4.351 \n",
      "String:ihehlo , epoch: 32, loss: 4.300 \n",
      "String:ihehlo , epoch: 33, loss: 4.253 \n",
      "String:ihello , epoch: 34, loss: 4.208 \n",
      "String:ihello , epoch: 35, loss: 4.166 \n",
      "String:ihello , epoch: 36, loss: 4.126 \n",
      "String:ihello , epoch: 37, loss: 4.088 \n",
      "String:ihello , epoch: 38, loss: 4.052 \n",
      "String:ihello , epoch: 39, loss: 4.018 \n",
      "String:ihello , epoch: 40, loss: 3.986 \n",
      "String:ihello , epoch: 41, loss: 3.955 \n",
      "String:ihello , epoch: 42, loss: 3.926 \n",
      "String:ihello , epoch: 43, loss: 3.898 \n",
      "String:ihello , epoch: 44, loss: 3.871 \n",
      "String:ihello , epoch: 45, loss: 3.846 \n",
      "String:ihello , epoch: 46, loss: 3.821 \n",
      "String:ihello , epoch: 47, loss: 3.798 \n",
      "String:ihello , epoch: 48, loss: 3.775 \n",
      "String:ihello , epoch: 49, loss: 3.754 \n",
      "String:ihello , epoch: 50, loss: 3.732 \n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "    pred=\"\"\n",
    "    for input, label in zip(inputs, labels):\n",
    "#        print(\"\\n Bf Shape output:\",input.size(),\"\\n Bf Shape label:\",label)\n",
    "        label=label.view(1)\n",
    "        hidden, output = model(hidden, input)\n",
    "#        print(\"\\n Shape output:\",np.shape(output),\"\\n Shape label:\",np.shape(label))\n",
    "        val, idx= output.max(1)\n",
    "#        print(val, idx)\n",
    "        pred+= getchar[idx.data]\n",
    "#        print(getchar[idx.data])\n",
    "        loss += criterion(output, label)\n",
    "    print(\"String:{} , epoch: {}, loss: {:.3f} \".format(pred, epoch + 1, loss.data))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
